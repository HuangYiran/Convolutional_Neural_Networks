{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图【猫】明白几种任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localization as Regression\n",
    "神经网络输出box coordinates，使用和实际box坐标的L2距离作为loss。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Recipe for Classification + Localization\n",
    "共用conv网络，根据不同的目的使用不同的输出网络，训练时分开训练。然而是先训练那个呢？conv网用不用多次训练呢？<br>\n",
    "图【分类+定位】<br>\n",
    "既可以在最后一层conv后加入regression head，也可以在全连接层之后，不明白其中区别在哪里。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localizing multiple objects\n",
    "但是只能是固定数量的，方法就是在regression head输出多个box conordinates，然后进行训练。\n",
    "\n",
    "### Human Pose Estimation\n",
    "原理是：人类都是有不同的肢体组合而成的，通过上面的localization model, 我们可以定位不同的肢体，然后通过不同肢体之间的位置关系，从而来推断观察对象的姿势。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window\n",
    "- Run classification + regression network at multiple locations on a high-resolution image\n",
    "- Convert fully-connected layers into convolutional layers for efficient computation\n",
    "- Combine classifier and regressor predictions across all scales for final prediction\n",
    "\n",
    "#### Overfeat\n",
    "核心还是使用AlexNet处理输入图片，输出和前面提到的Classification+Localization一样。重点是他多训练了一个regression网络。当输入图片不是(255,255)的时候，Overfeat会通过重复截取图上的(255,255)子图，经由classification+Localization网络得到一个多个不同的regression值。然后把这些值和其对应图片的坐标输入regression网络，得到最终的box coordinate。\n",
    "\n",
    "#### fully-connected layers into convolutional layers\n",
    "这里讲的有点怪，因为在讲conv的时候，都是讲二维的，但是这不是少了一维吗。感觉他应该是想表达，fully connected layer中神经元的个数，当成filter的个数，每个filter的size为，输入图片的size。因为每个神经元的权重都是可以训练的，所以应该是不同的filter。<br>\n",
    "这样做的好处是，当输入的图片大于训练用的图片的时候，他可以通过少量额外的计算，算出其他不同子窗口中图片的对应的输出。如下图，在test time中，可以方便的算出另外三个子窗口中的图片的regression值<br>\n",
    "图【Overfeat】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection\n",
    "和前面的Localization的主要区别在于，图中对象的个数是不确定的。<br>\n",
    "一种可行的做法是，不把detection当成regression问题，而是classification。设定固定window，在原图上依次滑动window，每次移动后，对window中的图片进行识别，如果能识别对象就标记出来，不能就跳过。window的大小有小到大，依次尝试。这方法有点暴力。（不知为何，突然想起了SOM）。\n",
    "\n",
    "#### Histogram of Oriented Gradients\n",
    "不懂，偷摘维基的。<br>\n",
    "方向梯度直方图（英语：Histogram of oriented gradient，简称HOG）是应用在计算机视觉和图像处理领域，用于目标检测的特征描述器。这项技术是用来计算局部图像梯度的方向信息的统计值。这种方法跟边缘方向直方图（edge orientation histograms）、尺度不变特征变换（scale-invariant feature transform descriptors）以及形状上下文方法（ shape contexts）有很多相似之处，但与它们的不同点是：HOG描述器是在一个网格密集的大小统一的细胞单元（dense grid of uniformly spaced cells）上计算，而且为了提高性能，还采用了重叠的局部对比度归一化（overlapping local contrast normalization）技术<br>\n",
    "HOG描述器最重要的思想是：在一副图像中，局部目标的表象和形状（appearance and shape）能够被梯度或边缘的方向密度分布很好地描述。具体的实现方法是：首先将图像分成小的连通区域，我们把它叫细胞单元。然后采集细胞单元中各像素点的梯度的或边缘的方向直方图。最后把这些直方图组合起来就可以构成特征描述器。为了提高性能，我们还可以把这些局部直方图在图像的更大的范围内（我们把它叫区间或block）进行对比度归一化（contrast-normalized），所采用的方法是：先计算各直方图在这个区间（block）中的密度，然后根据这个密度对区间中的各个细胞单元做归一化。通过这个归一化后，能对光照变化和阴影获得更好的效果。<br>\n",
    "与其他的特征描述方法相比，HOG描述器有很多优点。首先，由于HOG方法是在图像的局部细胞单元上操作，所以它对图像几何的（geometric）和光学的（photometric）形变都能保持很好的不变性，这两种形变只会出现在更大的空间领域上。其次，作者通过实验发现，在粗的空域抽样（coarse spatial sampling）、精细的方向抽样（fine orientation sampling）以及较强的局部光学归一化（strong local photometric normalization）等条件下，只要行人大体上能够保持直立的姿势，就容许行人有一些细微的肢体动作，这些细微的动作可以被忽略而不影响检测效果。综上所述，HOG方法是特别适合于做图像中的行人检测的\n",
    "图【HOG】\n",
    "图【DPM】\n",
    "\n",
    "#### Region Proposals\n",
    "上面的方法虽然加快了速度，但还是没有改变需要遍历的事实。下面这种方法，我们将尝试，预测对象的位置，然后只针对给出的位置，进行测试。一种进行推测的方法就是前面已经提到过了的agnostic Localization。（但还是无法解决数量不确定的问题啊？？）\n",
    "\n",
    "##### selective Search\n",
    "It start from pixels and you kind of merge adjacent pixels together if they have similar color and texture and form thes connected block like regions and then you merge up these regions to get bigger and bigger block part. And then for these different scales you could actually convert each of these block regions into a box by just drawing a box around it. So then by doing this over multiple scales you end up with a whole bunch of boxes around sort of a lot of block stuff in the image and it's reasonably fast to compute and actual the search space quite a lot.\n",
    "region proposal的方法有很多，见下图\n",
    "图【region proposal】\n",
    "\n",
    "#### Region based CNN(R-CNN)\n",
    "We have a input image, we are going to run a region proposal method like selective Search to get about maybe 20000 boxes different scale and position. And then we are going to warp these images to some fix size then run it forward through a CNN to classify and then this CNN is going to have a regression head and a classification head. (so that the regression head can correct the region proposals that were a little bit off.)\n",
    "\n",
    "###### R-CNN Training \n",
    "相较于一般的分类，R-CNN要考虑的东西会比较多，比如，输入图片猫头，可能也可以识别出是猫，但是在这里这种情况是不允许的。所以这里需要进行正方例训练（像Word2Vec的训练）。具体步骤如下：\n",
    "- Step1: Train(or download) a classification model for ImageNet(AlexNet)\n",
    "- Step2: fine-tune model for detection\n",
    "    - Instead of 1000 ImageNet classes, want 20 object classes + background\n",
    "    - Throw away final fully-connected layer, reinitialize from scratch\n",
    "    - Keep training model using positive/negative regions from detection images\n",
    "- Step3: Extract features\n",
    "    - Extract region proposals for all images\n",
    "    - For each region: warp to CNN input size, run forward through CNN, save pool5 features to disk\n",
    "    - Have a big hard drive: features are ~200GB for PASCAL dataset!\n",
    "- Step4: Train one binary SVM per class to classify region features\n",
    "- Step5: For each class, train a linear regression model to map from cached features to offsets to GT box to make up for \"slightly wrong\" proposals\n",
    "\n",
    "#### Evaluation\n",
    "使用mAP（Mean Average Precision）方法。值域是[0,100]，数值越高，结果越好。\n",
    "具体内容不懂。\n",
    "\n",
    "#### R-CNN Problem\n",
    "- Slow at test-time: need to run full forward pass of CNN for each region proposal\n",
    "- SVMs and regressors are post-hoc: CNN features not updated in response to SVMs and regressors\n",
    "- Complex multstage training pipeline\n",
    "\n",
    "#### Fast R-CNN\n",
    "We are just going to swap the order of extracting the region and running the CNN. So it's kind of this sliding window idea we saw with overfeat.\n",
    "图【fast R-CNN test】\n",
    "图【fast R-CNN training】\n",
    "R-CNN Problem: Slow at test-time due to independent forward passes of the CNN\n",
    "- Solution: Share computation of convolutional layers between proposals for an image\n",
    "R-CNN Problem: Post-hoc training and complex training pipeline\n",
    "- Solution: Just train the whole system end to end at once\n",
    "\n",
    "##### Core Technik in Fast R-CNN: Region of Interst Pooling\n",
    "the idea is that, we have the input image that's probably high resolution and we have this region proposal that's maybe out of selective search or edge boxes. and we can put our region in this high resolution image through our convolutional and pooling layers just fine because those are sort of scale invariant they scale up to different sizes of inputs(High-res conv features:C*H*W with region propoal) and now the problem is the fully connected layers from the pre-trained network are expecting low-res conv features: C*h*w. We solve this problem in a pretty straightforward way:<br>\n",
    "given this region proposal we're going to project it on to sort of the spatial part of that conv feature volume and now we going to divede the conv feature valume into a little grid that the downstream are expecting and we do max pooling within each of those grid cells.<br>\n",
    "图【region of Interest Pooling】\n",
    "the big problem is that these test time speeds don't include region proposals<br>\n",
    "The solution for it, is use NN for region proposal.\n",
    "\n",
    "#### Faster R-CNN\n",
    "The idea is pretty simple: instead to use some external method to compute region proposals they add the little thing called region proposal network that looks directly at these last convolutional features and is able to produce region proposals directly from that convolutional features map and once you have region proposal, you just do the same thing in fast R-CNN\n",
    "图【faster R-CNN】\n",
    "\n",
    "##### regioin Proposal Network\n",
    "前一个窗口大小固定，后一个使用模板。\n",
    "图【region Proposal Network1】\n",
    "图【region Proposal network2】\n",
    "\n",
    "##### Faster R-CNN training \n",
    "两种方法，一种分开训练，然后结合到一起，另一种就是end to end训练，不过会有4个loss\n",
    "\n",
    "一般使用的dataset有：\n",
    "图【Object detection dataset】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
