{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview \n",
    "- Segmentation\n",
    "    - Semantic Segmentation\n",
    "    - Instance Segmentation\n",
    "- (Soft) Attention\n",
    "    - Discrete locations\n",
    "    - Continuous locations(Spatial Transformers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Segmentation \n",
    "- Label every pixel\n",
    "- Don't differentiate instances\n",
    "\n",
    "![Semantic_Segmentation](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/Semantic_Segmentation.png)\n",
    "所以这种Segmentation并没有进行instance detection\n",
    "given the input image, we are going to take a little patch that sort of gives local information in the image. Then we are going to take this patch and feed it through some convolutional neural network. Now the CNN will actually classify the center pixel of the patch. So it's going to say that the center pixel of this patch actually is a cow。然后用这个神经网络对图中每个像素进行分类。这种方法花费很高。Now there are many patches in the image and it would be super expensive to run this network indenpendently for all of them. So in practice people use the same trick that we saw an object detection where you'll run this thing fully convolutionally and get all the outputs for the whole image all at once.(sliding window)<br>\n",
    "注意：The problem here is if your cnovolution network contains any kind of down sampling either through pooling or through striated convolutions now your output image will actually have a smaller spatial size then your input image. So that's something people need to work around when they're using this type of approach.<br>\n",
    "问题1: Patch提供的信息会否不足<br>\n",
    "回答1: when people use this network people actually have a separate offline refinement stage where they take this output and then feed it to some kind of graphical model to clean up the output a little bit. But just this raw input output model tends to work pretty well\n",
    "\n",
    "#### Multi-Scale\n",
    "1. Here we will take our input image and we will resize it to multiple different sizes(image pyramid: a common trick used in computer vision!!!). \n",
    "2. and now for these scales we're going to run it through a convolutional neural network that is goiing to predict these pixel wise labels for these different image for differnet resolution. Each these network actually has the same architecture and each of these output will have a different effective receptive field in the input due to the image pyramid. \n",
    "3. So now we've gotten these differently sized pixel labels for the image then we can take all of them and just do some offline upsampling to up sample those reponses to the same size as the input image. \n",
    "\n",
    "上面是神经网络部分，另外我们是用一些传统的图片处理方法获取相邻像素之间的连贯性信息，生成对应的super pixel或者是像素融合树（树形结构，说明那些像素之间可以进行融合merge）。最后结合上面获得的像素分类信息，得到最后结果。<br>\n",
    "So then this method actually takes sort of runs the image offline through these other more traditional image processing techniques to get either a set of super pixels or a tree saying which pixels ought to be merged together in the image. and they have this somewhat complicated process merging all these different things together.\n",
    "![Semantic_Segmentation](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/Multi_scale.png)\n",
    "\n",
    "#### Iterative refinement\n",
    "核心思想是使用同一个网络对图片进行多次分类。有点像rnn，把上一次得到的结果，作为输入，放入下一层网络中。因为每轮过后对应的image的size都会降低，所有在下一轮加入的image是origin image的down sampled version。<br>\n",
    "If you actually do more iterations of this same type of thing then hopefully it allows the network to sort of iteratively refine its output.<br>\n",
    "感觉有点怪怪的\n",
    "![Semantic_Segmentation](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/Refinement.png)\n",
    "\n",
    "#### Upsampling\n",
    "和前面相同的是输入图片，通过CNN得到对应的feature map。不同的是，这里不使用传统的up Sampling的方法，而是把upsampling作为一个可训练的网络加入到CNN网络结构中。<br>\n",
    "这个模型中另外一个有趣的地方是，有点像前面multi scale的思想，他不仅使用最后一层conv layer的输出，进行upsampling，他对整个CNN中多层conv layer的输出都进行upsampling，然后结合这里结果，来进行最后的输出。\n",
    "![Semantic_Segmentation](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/Upsampling1.png)\n",
    "![Semantic_Segmentation](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/Upsampling2.png)\n",
    "\n",
    "##### Learnable Unsampling: Deconvolution \n",
    "虽然这里叫deconvolution，但实际上并不是convolution的逆过程。<br>\n",
    "Here we want to take a low resolution input and produce a higher resolution output. So here may be a three by three deconvolution with stride 2 and a pad of one. Here you have you there by there filter and just copying it over to the output the only different is that the weights like this one scalar value of the weight in your input。<br>\n",
    "需要注意的点是：\n",
    "- 重叠则相加\n",
    "- stride指的是output的stride，input的默认是一。\n",
    "- 输入作为权值，用这个值乘以filter中各个位置对应的值作为输出。\n",
    "\n",
    "![Semantic_Segmentation](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/Deconvolution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InstanceSegmentation\n",
    "- Detect instances , give category, label pixels\n",
    "- simultaneous detection and segmentation(SDS)\n",
    "\n",
    "可以区分不同的instance<br>\n",
    "图【Instance Segmentation】\n",
    "过程和R-CNN类似\n",
    "\n",
    "#### Hypercolumns\n",
    "for region refinement, 输入是proposed segment。具体方法有点像前面提到的multi scale。首先从原图中截取对应Segmentation所在的box，然后把他扔进CNN（AlexNet这里），对每层feature map进行upsampling，然后组合他们，从而得到我们想要的proposed figure ground segmentation。最后生成的方法和第一个semantic Segmentation的方法一样，就是对每个pixel进行分类。\n",
    "![Semantic_Segmentation](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/Hypercolumns.png)\n",
    "\n",
    "### Cascades\n",
    "![Semantic_Segmentation](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/Cascades.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Models\n",
    "基本上和NLP的attention机制是一样的，不同的是这里会看的不是encoder隐藏层的数据，而是CNN中最后一层conv layer的输出数据，该数据中的每一张图片都代表着原图片的一种特征。比如最后输出的size为[7,7,512]，那么我们就相当于获得49个长为512的向量，通过每个向量的权重，最后得到一个长为512的向量，作为下一层神经元的输入。另外一点不同的是，这里并没有每一回合都和conv layer的输出进行比较计算权值，他的权值是通过上一层网络直接生成的（这点有点怪）。\n",
    "![Semantic_Segmentation](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/Soft_Attention.png)\n",
    "\n",
    "#### soft vs Hard Attention\n",
    "![Semantic_Segmentation](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/Soft_hard_attention.png)\n",
    "you can based on the reinforcement learning to actually train the model in this context where you want to select a single element of the input(hard attention)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arbitrary atention: Spatial Transformer Networks\n",
    "前面提到的attention，只能针对网格中的格子，比如前面的7*7网格。如何使得attention可以针对图片上不同的地方并且还可以保持他的可微性，比如给出一个任意的窗口表示attention。<br>\n",
    "![Semantic_Segmentation](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/Spatial_transformer_networks1.png)\n",
    "![Semantic_Segmentation](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/Spatial_transformer_networks2.png)\n",
    "从attention的角度来看，大体过程如下，从输出层获取对应attention的区域，通过Grid generator，计算出对应的原图上的区域，通过sampler双线插值返回对应的原图中的attention区域的信息，并把这个信息传递到输出网络。在这整个过程中可以通过调整Localization network的权值（即仿射的参数）使获得的信息，更有利于目标任务。\n",
    "\n",
    "不懂，一下大部分内容来自大饼博士的博客：<br>\n",
    "- bilinear interpolation:\n",
    "![Semantic_Segmentation](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/Bilinear_interpolation.png)\n",
    "- Spatial Transformer Networks，相当于在传统的一层Convolution中间，装了一个“插件”，可以使得传统的卷积带有了[裁剪]、[平移]、[缩放]、[旋转]等特性；理论上，作者希望可以减少CNN的训练数据量，以及减少做data argument，让CNN自己学会数据的形状变换。<br>\n",
    "- STN可以被安装在任意CNN的任意一层中<br>\n",
    "- V的计算公式和最前面双线性插值的示意图含义是一样的，只是因为在图像中，相邻两个点的坐标差是1，就没有分母部分了。而循环中大部分都没用的，只取相邻的四个点作为一个grid。\n",
    "- Grid generator和 3. Sampler是配合的，先通过V中坐标($x^{target},y^{target}$)以此找到它在U中的坐标，然后再通过双线性插值采样出真实的像素值，放到($x^{target},y^{target}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
