{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To summarize the Conv Layer:\n",
    "Accepts a volume of size $W_1\\ X\\ H_1\\ X\\ D_1$\n",
    "Requires four hyper parameters:\n",
    "- Number of filters K\n",
    "- their spatial extent F\n",
    "- the stride S\n",
    "- the amount of zero padding P\n",
    "\n",
    "一下几点是需要注意的：\n",
    "- K usually is chosen as a powers of two for computational reasons because if some libraries when they see powers of two in terms of number of say like your dimensions or number of kernels. sometimes they go into a special subroutine that is very very efficient. (F, S, P)搭配通常有(3,1,1),(5,1,2),(5,2,?),(1,1,0)<br>\n",
    "- The size of F is always odd, 因为这样可以使得目标对象两边获得的信息量对等。当然也可以使用偶数，这并没有强制要求。\n",
    "- 另外使用0进行padding的原因是，当你使用0的时候，因为我们padding只是为了顺滑程序，我们不想filter去重视我们新加入的内容。filter will not take those things into account. it's only taking your actual numbers into account.<br>\n",
    "- 还有一点是，我们通常默认会把图片resize为square。当然长方形的图片也是能够处理的就是了。<br>\n",
    "- 一般为了方便计算在同一层使用的filter的size都是相同的。<br>\n",
    "- 使用padding的一个最基本的原因是为了防止“lost size\", 一般的CNN都会有很多层（几十甚至上百），当你不是用padding的时候，每层得到的图片的大小将迅速下降，从而导致无法构成多层网络。<br>\n",
    "- 在这里有个学生点到了，只有第一层的conv层会接触到原图片，这也是后面会加入attention的主要原因。<br>\n",
    "- 计算每层参数数量的时候，通常为filter的数量乘以（filter的size + 1），加1表示bias。 从神经元的视角来看，每个filter相当于一个神经元，他的输入的数量为filter size的大小（即W等于size），输出为1。但其实这种说法是有点不太准确的，因为这个filter会遍历整个图片，而神经元的输入对象是固定的，因此我们更加愿意，用多个神经元来代表一个filter（具体神经元的数量由图片的size和stride来决定），每个神经元集中处理同一个地方的输入，并且这些神经元都共享同一个W（也因为共享，所以参数数量并没有增加）。<br>\n",
    "- 另一个应该注意的是这里的size是一个三维向量。他还包括了channel。由这里也可以看到前面的(1,1,0)（size 1，stride 1，padding 0的conv layer）的实际作用对象是各个channel。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use case讲得挺多的，下面几点是比较好玩的：\n",
    "- VGGNet中关于memory的说法挺有趣的，还有就是用average pool加FC，替代单纯的FC的小技巧。\n",
    "- GoogLeNet的inception module\n",
    "- ResNet中的residual\n",
    "- alphaGo中（19，19，48）中的48。他说是48中有用的feature，会是神马呢？另外棋盘变图片就是这么简单，这么任性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LeNet](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/CNN-LeNet5.png)\n",
    "![ZFNet](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/CNN-ZFNet.png)\n",
    "![ALexNet](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/CNN-ALexNet.png)\n",
    "![CGGNet](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/CNN-CGGNet.png)\n",
    "![GoogLeNet](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/CNN-GoogLetNet.png)\n",
    "![ResNet1](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/CNN-ResNet1.png)\n",
    "![ResNet2](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/CNN-ResNet2.png)\n",
    "![ResNet3](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/CNN-ResNet3.png)\n",
    "![ResNet4](https://raw.githubusercontent.com/HuangYiran/Convolutional_Neural_Networks/master/CNN-ResNet4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
